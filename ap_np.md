---
layout: default
title: Appropriate and Inappropriate Uses 
nav_order: 6
---

Appropriate uses for the model include the following:
- One can use the component that identifies hostile text, train it with general data from all age groups, and use that feature to flag hostile text that may be harmful in other contexts. This is especially useful in cases related to canceling on social media or online political turmoils. 
- It is also appropriate to use this model’s results for campaigns that aim to provide resources related to cyberbullying to ensure that the allocation of resources is efficient. For example, a national nonprofit organization could have an immersive educational program they want to integrate into certain schools’ curricula. This model could help the nonprofit identify which demographics in which locations experience cyberbullying the most and target schools that match the model’s results. 
- I need help on this, i'm not sure what else to include

Inappropriate uses for the model are generally uses that take the results from the model and attempt to apply them to an external problem due to a potential correlation of a shared attribute. Inappropriate uses for the model include the following:
- For example, it is possible for a well-intentioned initiative to assume that the individuals identified by the model are also more likely to suffer from mental health issues since their vulnerability would also make them more of a target for cyberbullying. However, this is a weak correlation and unproven to be significant since mental health is not accounted for in any of the predictors in the model. Therefore, this initiative will likely cause unjustified actions and lead to an unjustified allocation of resources. 
- Another potential inappropriate use is using the aggregate outputs of the model, essentially a collection of accounts that are flagged or at risk for cyberbullying, for malicious purposes. Even if external parties had limited access to this data, teams within Instagram could potentially use this information for other platform purposes that wouldn’t be best for the potential victims of cyberbullying. For example, a research team could decide to explore whether there is a relationship between being at risk for cyberbullying and doom scrolling. This is an inappropriate use because it treats the individuals as means. 
- It is also inappropriate for the trends about cyberbullying learned throughout this project to be generalized since this model only targets teenagers on Instagram. For example, if this model shouldn’t be used by law enforcement to flag accounts of people who are vulnerable to being recruited into political insurrections or being radicalized because even though those targets and the cyberbullying victims have much overlap, there are substantial differences in demographics, colloquialism, and context not included in the training data. 
